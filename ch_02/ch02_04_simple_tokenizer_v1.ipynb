{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7eed8765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "55be4996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/media/mistertandon/DATA/git_repos/ai/01-llm-rsbt/settings')\n",
    "\n",
    "with open('the-verdict.txt', 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9aee213e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "34e67998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/media/mistertandon/DATA/git_repos/ai/01-llm-rsbt/ch_02')\n",
    "\n",
    "from ch02_03_verdict_vocab import Vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2b182829",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_builder = Vocabulary(raw_text)\n",
    "vocab_builder.tokenize(r'([,.:;?_!\"()\\']|--|\\s)')\n",
    "vocab_builder.get_unique_tokens()\n",
    "vocab_builder.sort_unique_tokens()\n",
    "vocab_builder.build_vocabulary()\n",
    "\n",
    "vocabulary_v1 = vocab_builder.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ee9fc01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ('yet', 1125)\n",
      "1: ('you', 1126)\n",
      "2: ('younger', 1127)\n",
      "3: ('your', 1128)\n",
      "4: ('yourself', 1129)\n",
      "1130\n"
     ]
    }
   ],
   "source": [
    "for idx, item in enumerate(list(vocabulary_v1.items())[-5:]):\n",
    "    print(f\"{idx}: {item}\")\n",
    "\n",
    "print(len(vocabulary_v1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4b65ec4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {token_id:token for token, token_id in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "        token_ids = [self.str_to_int[token] for token in preprocessed]\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        text = ' '.join([self.int_to_str[token_id] for token_id in token_ids])\n",
    "        text = re.sub(r'\\s+([,.?_!\"()\\'])', r'\\1', text)\n",
    "\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "feab902d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocabulary_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5bc61aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n",
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "raw_text_v1 = \"\"\"\"It's the last he painted, you know,\"\n",
    "Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "\n",
    "raw_text_token_ids_v1 = tokenizer.encode(raw_text_v1)\n",
    "print(\"Token IDs:\", raw_text_token_ids_v1)\n",
    "print(tokenizer.decode(raw_text_token_ids_v1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c2a49c2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[109], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m test_text_v1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, do you like tea?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m token_ids_v1 \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_text_v1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToken IDs:\u001b[39m\u001b[38;5;124m\"\u001b[39m, token_ids_v1)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(token_ids_v1))\n",
      "Cell \u001b[0;32mIn[106], line 10\u001b[0m, in \u001b[0;36mSimpleTokenizerV1.encode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m      7\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[1;32m      8\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[0;32m---> 10\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_to_int[token] \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m token_ids\n",
      "Cell \u001b[0;32mIn[106], line 10\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[1;32m      8\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[0;32m---> 10\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m token_ids\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Hello'"
     ]
    }
   ],
   "source": [
    "test_text_v1 = \"Hello, do you like tea?\"\n",
    "\n",
    "token_ids_v1 = tokenizer.encode(test_text_v1)\n",
    "print(\"Token IDs:\", token_ids_v1)\n",
    "print(tokenizer.decode(token_ids_v1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f990909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce8102e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
